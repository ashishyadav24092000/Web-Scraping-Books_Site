{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f97a251",
   "metadata": {},
   "source": [
    "## Site used from where the data is scraped::\n",
    "    Coding Help --> https://data36.com/how-to-become-a-data-scientist/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db31d1",
   "metadata": {},
   "source": [
    "## In this we will scrape the data about bestseller books like ::\n",
    "    1. Title\n",
    "    2. Book format(paperback,harback etc.)\n",
    "    3. Price\n",
    "    4. Year of Publishing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e0d69",
   "metadata": {},
   "source": [
    "## Let's Scrape multiple page web in one loop:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87742ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import relevant libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed7a9e9",
   "metadata": {},
   "source": [
    "## NOTE::\n",
    "    1. previous_sibling is used to find the previous element of the given element.\n",
    "    2. next_sibling is used to find the next element of the given element.\n",
    "    3. previous_siblings is used to find all previous element of the given element.\n",
    "    4. next_siblings is used to find all next element of the given element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT NOTE::\n",
    "##1. while .find() returns only one element,\n",
    "##2. find_all() returns a list of elements, which means you can iterate over it\n",
    "\n",
    "page = 1\n",
    "bestsellers = []\n",
    "while(page!=35):\n",
    "    basic_url = f\"https://www.bookdepository.com/bestsellers?page={page}\"\n",
    "    response = requests.get(basic_url)\n",
    "    html = response.content\n",
    "    soup = bs(html,'html.parser')\n",
    "    for book in soup.find_all('div',class_ = \"book-item\"):\n",
    "        bestseller_book = {}         ## a dictionary to store one book info in each iteration\n",
    "        bestseller_book[\"title\"] = book.h3.get_text(strip=True)         ## strip is set True for removing the whitespaces  \n",
    "        bestseller_book[\"format\"] = book.find('p',class_=\"format\").get_text()\n",
    "        \n",
    "        ## Use try, except for handling errors since all of the books don't have their \"year\" and \"original Price\" available on the website\n",
    "        \n",
    "        ## Handling missing 'year' error\n",
    "        try:\n",
    "            bestseller_book[\"year\"] = book.find('p',class_=\"published\").get_text()[-4:]\n",
    "        except AttributeError:\n",
    "            bestseller_book[\"year\"] = \"\"\n",
    "        \n",
    "        \n",
    "        ## Handling missing 'original_price' error\n",
    "        price = book.find('p',class_='price')\n",
    "        try:\n",
    "            original_price = price.find('span',class_='rrp')\n",
    "        except AttributeError:\n",
    "            bestseller_book[\"price\"] = \"\"\n",
    "        else:\n",
    "            if original_price:\n",
    "                current_price = str(original_price.previousSibling).strip()\n",
    "                current_price = current_price.replace(\",\",\"\")\n",
    "                current_price = float(current_price.replace(\"₹\",\"\"))\n",
    "            else:\n",
    "                current_price = str(price.get_text(strip=True)).replace(\",\",\"\")\n",
    "                current_price = float(current_price.replace(\"₹\",\"\"))\n",
    "            bestseller_book[\"price\"] = current_price\n",
    "        bestsellers.append(bestseller_book)\n",
    "    page += 1\n",
    "\n",
    "##column_titles = [\"Title of book\",\"Book-Format\",\"Publishing-Year\",\"Price\"]\n",
    "##bestsellers.insert(0,column_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1128b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe_books = pd.DataFrame(bestsellers)\n",
    "dataframe_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959cb808",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating an excel file for the data frame\n",
    "file_name = \"Boooks Depository.xlsx\"\n",
    "\n",
    "dataframe_books.to_excel(file_name,encoding=\"utf-8\")\n",
    "print(\"Success!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea84b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Books of year 2000\n",
    "dataframe_books[dataframe_books['year'] == \"2000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b888c7",
   "metadata": {},
   "source": [
    "## Note two ways to request data from internet or site is Shown below, so don't get confused when first one or the second one is used::)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_url = \"https://www.bookdepository.com/bestsellers\"\n",
    "response = requests.get(basic_url)\n",
    "response        ## It returns the response status value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "url = \"https://www.bookdepository.com/bestsellers\"\n",
    "response = urlopen(url)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
